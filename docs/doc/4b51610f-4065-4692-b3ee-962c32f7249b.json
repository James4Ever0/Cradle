{
    "summary": "The code provides functions for image captioning and object detection, utilizing Singleton pattern to handle annotations and categorize detections. It also writes annotated frames and processes minimap images.",
    "details": [
        {
            "comment": "This code is defining a function called \"unique_predict\" that takes an image, caption, box threshold, and model as inputs. It ensures the input caption ends with a period, moves the model and image to specified device (CPU or GPU), performs inference using the model, extracts prediction logits and boxes, applies a mask based on the box threshold, and returns only the relevant logits.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/gd_provider.py\":0-38",
            "content": "import math\nimport cv2\nimport torch\nimport numpy as np\nfrom torchvision.ops import box_convert\nfrom groundingdino.util.inference import load_model, load_image\nfrom cradle.gameio.lifecycle.ui_control import annotate_with_coordinates, segment_minimap\nfrom cradle.utils import Singleton\nfrom cradle.log import Logger\nfrom cradle import constants\nlogger = Logger()\ndef unique_predict(\n        model,\n        image: torch.Tensor,\n        caption: str,\n        box_threshold: float,\n        device: str = \"cuda\",\n):\n    caption = caption.lower().strip()\n    if not caption.endswith(\".\"):\n        caption = caption + \" .\"\n    model = model.to(device)\n    image = image.to(device)\n    with torch.no_grad():\n        outputs = model(image[None], captions=[caption])\n    prediction_logits = outputs[\"pred_logits\"].cpu().sigmoid()[0]  # prediction_logits.shape = (nq, 256)\n    prediction_boxes = outputs[\"pred_boxes\"].cpu()[0]  # prediction_boxes.shape = (nq, 4)\n    mask = prediction_logits.max(dim=1)[0] > box_threshold\n    logits = prediction_logits[mask]  # logits.shape = (n, 256)"
        },
        {
            "comment": "The code is for image captioning. It takes an input text and the logits from a detection model, and then separates the text into individual words. For each word, it calculates the cumulative probability of that word appearing in the input text. If the cumulative probability exceeds the current maximum, it updates the maximum and stores the corresponding phrase. Finally, it returns the prediction boxes, the logits with the highest value for each input, and a list of separated phrases. The code is part of the GdProvider class, which uses a Singleton design pattern to ensure only one instance exists at any time.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/gd_provider.py\":39-70",
            "content": "    boxes = prediction_boxes[mask]  # boxes.shape = (n, 4)\n    # Modified version: recognize seperation and choose the best one with highest probability\n    phrases = []\n    input_text = caption.split()\n    for logit in logits:\n        prob = logit[logit > 0][1:-1]\n        max_prob, cum_prob, pre_i, label = 0, 0, 0, ''\n        for i, (c, p) in enumerate(zip(input_text, prob)):\n            if c == '.':\n                if cum_prob > max_prob:\n                    max_prob = cum_prob\n                    label = ' '.join(input_text[pre_i:i])\n                cum_prob = 0\n                pre_i = i + 1\n            else:\n                cum_prob += p\n        phrases.append(label)\n    return boxes, logits.max(dim=1)[0], phrases\nclass GdProvider(metaclass=Singleton):\n    def __init__(self):\n        self.detect_model = None\n        try:\n            self.detect_model = load_model(\"./cache/GroundingDINO_SwinB_cfg.py\", \"./cache/groundingdino_swinb_cogcoor.pth\")\n        except Exception as e:\n            logger.error(f\"Faile"
        },
        {
            "comment": "The code defines a function `detect` to detect objects in an image and return the coordinates, logits (confidence scores), and recognized phrases. Another function `save_annotate_frame` saves annotated frames by removing the main character from detected boxes if it's mentioned in the text prompt.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/gd_provider.py\":70-102",
            "content": "d to load the grounding model. Make sure you follow the instructions on README to download the necessary files.\\n{e}\")\n    def detect(self, image_path,\n                  text_prompt=\"wolf .\",\n                  box_threshold=0.4,\n                  device='cuda',\n                  ):\n        image_source, image = load_image(image_path)\n        boxes, logits, phrases = unique_predict(\n            model=self.detect_model,\n            image=image,\n            caption=text_prompt,\n            box_threshold=box_threshold,\n            device=device\n        )\n        return image_source, boxes, logits, phrases\n    def save_annotate_frame(self, image_source, boxes, logits, phrases, text_prompt, cur_screen_shot_path):\n        # Remove the main character itself from boxes\n        if \"person\" in text_prompt.lower():\n            if len(boxes) > 1:\n                index = 0\n                dis = 1.5\n                for i in range(len(boxes)):\n                    down_mid = (boxes[i, 0], boxes[i, 1] + boxes[i, 3] / 2)\n                    distance = torch.sum(torch.abs(torch.tensor(down_mid) - torch.tensor((0.5, 1.0))))"
        },
        {
            "comment": "This code block is responsible for handling different scenarios involving boxes and logits. If there are multiple boxes, it rearranges them and the corresponding logits by comparing their distances to a given distance value. Then, it generates an annotated frame using the adjusted boxes and logits. If there's only one box, it performs similar operations but on sliced arrays. Otherwise, it simply creates an annotated frame without any rearrangement. The final step is writing the annotated frame to a file.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/gd_provider.py\":104-125",
            "content": "                    if distance < dis:\n                        dis = distance\n                        index = i\n                boxes_ = torch.cat([boxes[:index], boxes[index + 1:]])\n                logits_ = torch.cat([logits[:index], logits[index + 1:]])\n                phrases.pop(index)\n                annotated_frame = annotate_with_coordinates(image_source=image_source, boxes=boxes_[:,:], logits=logits_[:], phrases=phrases)\n                cv2.imwrite(cur_screen_shot_path, annotated_frame)\n            elif len(boxes)==1:\n                phrases.pop(0)\n                boxes_ = torch.tensor(boxes[1:])\n                logits_ = torch.tensor(logits[1:])\n                annotated_frame = annotate_with_coordinates(image_source=image_source, boxes=boxes_[:,:], logits=logits_[:], phrases=phrases)\n                cv2.imwrite(cur_screen_shot_path, annotated_frame)\n            else:\n                annotated_frame = annotate_with_coordinates(image_source=image_source, boxes=boxes[:,:], logits=logits[:], phrases=phrases)"
        },
        {
            "comment": "This code segment is part of a function in the \"gd_provider.py\" file. It first checks if a current screen shot path exists, and if so, writes an annotated frame to it. If not, it annotates an image based on input parameters like image source, bounding boxes, logits, and phrases, then writes the annotated frame to the specified path. The code segment also includes a function to process a minimap image by detecting red, yellow points, and regions using the detected box coordinates, logits, and phrases, and calculates the angle needed to turn based on these points.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/gd_provider.py\":126-144",
            "content": "                cv2.imwrite(cur_screen_shot_path, annotated_frame)\n        else:\n            annotated_frame = annotate_with_coordinates(image_source=image_source, boxes=boxes[:,:], logits=logits[:], phrases=phrases)\n            cv2.imwrite(cur_screen_shot_path, annotated_frame)\n    # Process current minimap for detect red points, yellow points and yellow region. return the angle to turn.\n    def process_minimap_targets(self, image_path):\n        minimap_image, boxes, logits, phrases = self.detect(image_path=segment_minimap(image_path),\n                                                                text_prompt=constants.GD_PROMPT,\n                                                                box_threshold=0.29, device='cuda')\n        get_theta = lambda x0, y0, x, y:math.degrees(math.atan2(x - x0, y0 - y))\n        h, w, _ = minimap_image.shape\n        xyxy = box_convert(boxes=boxes.detach().cpu() * torch.Tensor([w, h, w, h]), in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy().astype(int)\n        minimap_d"
        },
        {
            "comment": "This code is filtering and categorizing detected objects based on size and color. It excludes large detections, merges yellow points into a region if they exceed certain size thresholds, calculates the center position of each detection box, and stores the object's orientation (theta) in a dictionary for further processing. The function returns a dictionary with categorized detections.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/gd_provider.py\":144-164",
            "content": "etection_objects = {constants.RED_POINTS: [], constants.YELLOW_POINTS: [], constants.YELLOW_REGION: []}\n        for detect_xyxy, detect_object, detect_confidence in zip(xyxy, phrases, logits):\n            # Exclude too large detections\n            if detect_xyxy[2] - detect_xyxy[0] > 0.8 * w and detect_xyxy[3] - detect_xyxy[1] > 0.8 * h:\n                continue\n            if detect_object == constants.YELLOW_POINTS and (detect_xyxy[2] - detect_xyxy[0] > 0.1 * w or detect_xyxy[3] - detect_xyxy[1] > 0.1 * h):\n                detect_object = constants.YELLOW_REGION\n            tgt_x = int((detect_xyxy[0] + detect_xyxy[2]) / 2)  # center of the box\n            tgt_y = int((detect_xyxy[1] + detect_xyxy[3]) / 2)\n            theta = get_theta(h // 2, w // 2, tgt_x, tgt_y)\n            minimap_detection_objects[detect_object].append(dict(\n                theta=theta,\n            ))\n        return minimap_detection_objects"
        }
    ]
}