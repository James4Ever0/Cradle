{
    "summary": "The code initializes OpenAI provider, generates safe embeddings, provides text embedding and chat completion functions using GPT-4 vision model, manages API requests, handles exceptions, logs completion info, calculates token requirements, and takes user input with error handling for unexpected inputs and different input types.",
    "details": [
        {
            "comment": "This code imports various libraries and defines constants, dictionaries, and variables for configuring an OpenAI provider in Cradle. It sets up the MAX_TOKENS dictionary, PROVIDER_SETTING_KEY_VAR, PROVIDER_SETTING_EMB_MODEL, PROVIDER_SETTING_COMP_MODEL, PROVIDER_SETTING_IS_AZURE, and PROVIDER_SETTING_BASE_VAR variables for configuring the OpenAI provider.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":0-46",
            "content": "import os\nfrom typing import (\n    Any,\n    Dict,\n    List,\n    Literal,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Union,\n)\nimport json\nimport re\nimport io\nimport asyncio\nimport backoff\nimport tiktoken\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom openai import OpenAI, AzureOpenAI, APIError, RateLimitError, APITimeoutError\nfrom cradle import constants\nfrom cradle.provider import LLMProvider, EmbeddingProvider\nfrom cradle.config import Config\nfrom cradle.log import Logger\nfrom cradle.utils.json_utils import load_json\nfrom cradle.utils.encoding_utils import encode_base64, decode_base64\nfrom cradle.utils.file_utils import assemble_project_path\nfrom cradle.utils.string_utils import hash_text_sha256\nconfig = Config()\nlogger = Logger()\nMAX_TOKENS = {\n    \"gpt-3.5-turbo-0301\": 4097,\n    \"gpt-3.5-turbo-0613\": 4097,\n    \"gpt-3.5-turbo-16k-0613\": 16385,\n}\nPROVIDER_SETTING_KEY_VAR = \"key_var\"\nPROVIDER_SETTING_EMB_MODEL = \"emb_model\"\nPROVIDER_SETTING_COMP_MODEL = \"comp_model\"\nPROVIDER_SETTING_IS_AZURE = \"is_azure\"\nPROVIDER_SETTING_BASE_VAR = \"base_var\"       # Azure-speficic setting"
        },
        {
            "comment": "This Python class, OpenAIProvider, is a wrapper for a given model that uses Azure-specific settings such as API version and models. The class has attributes for LLM and Embedding providers, along with other configuration parameters like chunk size, embedding context length, timeout, Tiktoken model name, and options to skip empty strings. It initializes the class instance and allows retries when initializing the provider using a provider configuration.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":47-82",
            "content": "PROVIDER_SETTING_API_VERSION = \"api_version\" # Azure-speficic setting\nPROVIDER_SETTING_DEPLOYMENT_MAP = \"models\"   # Azure-speficic setting\nclass OpenAIProvider(LLMProvider, EmbeddingProvider):\n    \"\"\"A class that wraps a given model\"\"\"\n    client: Any = None\n    llm_model: str = \"\"\n    embedding_model: str = \"\"\n    allowed_special: Union[Literal[\"all\"], Set[str]] = set()\n    disallowed_special: Union[Literal[\"all\"], Set[str], Sequence[str]] = \"all\"\n    chunk_size: int = 1000\n    embedding_ctx_length: int = 8191\n    request_timeout: Optional[Union[float, Tuple[float, float]]] = None\n    tiktoken_model_name: Optional[str] = None\n    \"\"\"Whether to skip empty strings when embedding or raise an error.\"\"\"\n    skip_empty: bool = False\n    def __init__(self) -> None:\n        \"\"\"Initialize a class instance\n        Args:\n            cfg: Config object\n        Returns:\n            None\n        \"\"\"\n        self.retries = 5\n    def init_provider(self, provider_cfg ) -> None:\n        self.provider_cfg = self._parse_config(provider_cfg)"
        },
        {
            "comment": "This function parses a provider configuration file and initializes the appropriate OpenAI client, based on whether the config is for Azure or not. It loads the embedding model and language model specified in the config and assigns them to instance variables.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":85-116",
            "content": "    def _parse_config(self, provider_cfg) -> dict:\n        \"\"\"Parse the config object\"\"\"\n        conf_dict = dict()\n        if isinstance(provider_cfg, dict):\n            conf_dict = provider_cfg\n        else:\n            path = assemble_project_path(provider_cfg)\n            conf_dict = load_json(path)\n        key_var_name = conf_dict[PROVIDER_SETTING_KEY_VAR]\n        if conf_dict[PROVIDER_SETTING_IS_AZURE]:\n            key = os.getenv(key_var_name)\n            endpoint_var_name = conf_dict[PROVIDER_SETTING_BASE_VAR]\n            endpoint = os.getenv(endpoint_var_name)\n            self.client = AzureOpenAI(\n                api_key = key,\n                api_version = conf_dict[PROVIDER_SETTING_API_VERSION],\n                azure_endpoint = endpoint\n            )\n        else:\n            key = os.getenv(key_var_name)\n            self.client = OpenAI(api_key=key)\n        self.embedding_model = conf_dict[PROVIDER_SETTING_EMB_MODEL]\n        self.llm_model = conf_dict[PROVIDER_SETTING_COMP_MODEL]\n        return conf_dict"
        },
        {
            "comment": "The code defines a property _emb_invocation_params, which gets the model name from self.embedding_model and sets it as openai_args['model']. If the provider is Azure, it also retrieves the deployment ID for the specified model using _get_azure_deployment_id_for_model() method and sets it in openai_args. The embed_with_retry() function uses backoff with exponential backoff to retry the embedding call with maximum retries and max value, while handling exceptions such as APIError, RateLimitError, and APITimeoutError.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":118-150",
            "content": "    @property\n    def _emb_invocation_params(self) -> Dict:\n        openai_args = {\n            \"model\": self.embedding_model,\n        }\n        if self.provider_cfg[PROVIDER_SETTING_IS_AZURE]:\n            engine = self._get_azure_deployment_id_for_model(self.embedding_model)\n            openai_args = {\n                \"model\": self.embedding_model,\n            }\n        return openai_args\n    def embed_with_retry(self, **kwargs: Any) -> Any:\n        \"\"\"Use backoff to retry the embedding call.\"\"\"\n        @backoff.on_exception(\n            backoff.expo,\n            (\n                APIError,\n                RateLimitError,\n                APITimeoutError,\n            ),\n            max_tries=self.retries,\n            max_value=10,\n            jitter=None,\n        )\n        def _embed_with_retry(**kwargs: Any) -> Any:\n            response = self.client.embeddings.create(**kwargs)\n            if any(len(d.embedding) == 1 for d in response.data):\n                raise RuntimeError(\"OpenAI API returned an empty embedding\")"
        },
        {
            "comment": "This function generates safe embeddings for a list of texts using the OpenAI Embeddings class. It checks if the tiktoken package is installed, and if not, raises an ImportError. The function encodes each text using the encoding corresponding to the provided model name (or falls back to cl100k_base). It then returns a list of lists, where each inner list represents the embeddings for each text in the input.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":151-181",
            "content": "            return response\n        return _embed_with_retry(**kwargs)\n    def _get_len_safe_embeddings(\n        self,\n        texts: List[str],\n    ) -> List[List[float]]:\n        embeddings: List[List[float]] = [[] for _ in range(len(texts))]\n        try:\n            import tiktoken\n        except ImportError:\n            raise ImportError(\n                \"Could not import tiktoken python package. \"\n                \"This is needed in order to for OpenAIEmbeddings. \"\n                \"Please install it with `pip install tiktoken`.\"\n            )\n        tokens = []\n        indices = []\n        model_name = self.tiktoken_model_name or self.embedding_model\n        try:\n            encoding = tiktoken.encoding_for_model(model_name)\n        except KeyError:\n            logger.warn(\"Warning: model not found. Using cl100k_base encoding.\")\n            model = \"cl100k_base\"\n            encoding = tiktoken.get_encoding(model)\n        for i, text in enumerate(texts):\n            token = encoding.encode(\n                text,"
        },
        {
            "comment": "This code is responsible for tokenizing and embedding text into embeddings. It splits the input text into chunks, then retrieves the embeddings using an external API, and finally appends the embeddings to a list of results, handling empty batches if necessary. The chunk size and embedding context length are specified as parameters.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":182-205",
            "content": "                allowed_special=self.allowed_special,\n                disallowed_special=self.disallowed_special,\n            )\n            for j in range(0, len(token), self.embedding_ctx_length):\n                tokens.append(token[j : j + self.embedding_ctx_length])\n                indices.append(i)\n        batched_embeddings: List[List[float]] = []\n        _chunk_size = self.chunk_size\n        _iter = range(0, len(tokens), _chunk_size)\n        for i in _iter:\n            response = self.embed_with_retry(\n                input=tokens[i : i + self.chunk_size],\n                **self._emb_invocation_params,\n            )\n            batched_embeddings.extend(r.embedding for r in response.data)\n        results: List[List[List[float]]] = [[] for _ in range(len(texts))]\n        num_tokens_in_batch: List[List[int]] = [[] for _ in range(len(texts))]\n        for i in range(len(indices)):\n            if self.skip_empty and len(batched_embeddings[i]) == 1:\n                continue\n            results[indices[i]].append(batched_embeddings[i])"
        },
        {
            "comment": "This code snippet defines a class method that calculates embeddings for a list of texts. It uses the OpenAI embedding API and implements length-safe embedding to handle longer texts. In case there are no results from the API, it falls back to a default embedding calculation. The calculated embeddings are returned as a list.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":206-235",
            "content": "            num_tokens_in_batch[indices[i]].append(len(tokens[i]))\n        for i in range(len(texts)):\n            _result = results[i]\n            if len(_result) == 0:\n                average = self.embed_with_retry(\n                    input=\"\",\n                    **self._emb_invocation_params,\n                ).data[0].embedding\n            else:\n                average = np.average(_result, axis=0, weights=num_tokens_in_batch[i])\n            embeddings[i] = (average / np.linalg.norm(average)).tolist()\n        return embeddings\n    def embed_documents(\n        self,\n        texts: List[str],\n    ) -> List[List[float]]:\n        \"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\n        Args:\n            texts: The list of texts to embed.\n        Returns:\n            List of embeddings, one for each text.\n        \"\"\"\n        # NOTE: to keep things simple, we assume the list may contain texts longer\n        #       than the maximum context and use length-safe embedding function.\n        return self._get_len_safe_embeddings(texts)"
        },
        {
            "comment": "This code defines functions for embedding text using OpenAI's embedding endpoint, retrieving the embedding dimension based on the model, and creating a chat completion using the OpenAI API. The `embed_query` function takes a text input and returns its corresponding embedding, while `get_embedding_dim` retrieves the dimension of the chosen embedding model. The `create_completion` function generates a chat completion using messages, optional model selection, temperature, seed, and maximum token count.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":238-272",
            "content": "    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\n        Args:\n            text: The text to embed.\n        Returns:\n            Embedding for the text.\n        \"\"\"\n        return self.embed_documents([text])[0]\n    def get_embedding_dim(self) -> int:\n        \"\"\"Get the embedding dimension.\"\"\"\n        if self.embedding_model == \"text-embedding-ada-002\":\n            embedding_dim = 1536\n        else:\n            raise ValueError(f\"Unknown embedding model: {self.embedding_model}\")\n        return embedding_dim\n    def create_completion(\n        self,\n        messages: List[Dict[str, str]],\n        model: str | None = None,\n        temperature: float = config.temperature,\n        seed: int = config.seed,\n        max_tokens: int = config.max_tokens,\n    ) -> Tuple[str, Dict[str, int]]:\n        \"\"\"Create a chat completion using the OpenAI API\n        Supports both GPT-4 and GPT-4V).\n        Example Usage:\n        image_path = \"path_to_your_image.jpg\""
        },
        {
            "comment": "This code is creating a chat completion using the GPT-4 vision model. It encodes an image into base64 format, then creates a message with both text and image URL for the model to analyze. The model's response will be stored in 'response'. If debug mode is enabled, it logs information about the request being made to the model.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":273-307",
            "content": "        base64_image = encode_image(image_path)\n        response, info = self.create_completion(\n            model=\"gpt-4-vision-preview\",\n            messages=[\n              {\n                \"role\": \"user\",\n                \"content\": [\n                  {\n                    \"type\": \"text\",\n                    \"text\": \"What\u2019s in this image?\"\n                  },\n                  {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                      \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                    }\n                  }\n                ]\n              }\n            ],\n        )\n        \"\"\"\n        if model is None:\n            model = self.llm_model\n        if config.debug_mode:\n            logger.debug(f\"Creating chat completion with model {model}, temperature {temperature}, max_tokens {max_tokens}\")\n        else:\n            logger.write(f\"Requesting {model} completion...\")\n        @backoff.on_exception(\n            backoff.constant,\n            (\n                APIError,"
        },
        {
            "comment": "This code defines a function that sends a request to the OpenAI API to generate responses. It uses retry logic with rate limiting and timeout handling, and handles Azure and non-Azure providers differently.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":308-336",
            "content": "                RateLimitError,\n                APITimeoutError),\n            max_tries=self.retries,\n            interval=10,\n        )\n        def _generate_response_with_retry(\n            messages: List[Dict[str, str]],\n            model: str,\n            temperature: float,\n            seed: int = None,\n            max_tokens: int = 512,\n        ) -> Tuple[str, Dict[str, int]]:\n            \"\"\"Send a request to the OpenAI API.\"\"\"\n            if self.provider_cfg[PROVIDER_SETTING_IS_AZURE]:\n                response = self.client.chat.completions.create(model=model,\n                messages=messages,\n                temperature=temperature,\n                seed=seed,\n                max_tokens=max_tokens,)\n            else:\n                response = self.client.chat.completions.create(model=model,\n                messages=messages,\n                temperature=temperature,\n                seed=seed,\n                max_tokens=max_tokens,)\n            if response is None:\n                logger.error(\"Failed to get a response from OpenAI. Try again.\")"
        },
        {
            "comment": "This code retrieves a response from an OpenAI model, handles potential errors with retry logic, logs information about the response, and returns the generated message and usage details. It also provides an asynchronous version of the function for improved efficiency.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":337-370",
            "content": "                logger.double_check()\n            message = response.choices[0].message.content\n            info = {\n                \"prompt_tokens\" : response.usage.prompt_tokens,\n                \"completion_tokens\" : response.usage.completion_tokens,\n                \"total_tokens\" : response.usage.total_tokens,\n                \"system_fingerprint\" : response.system_fingerprint,\n            }\n            logger.write(f'Response received from {model}.')\n            return message, info\n        return _generate_response_with_retry(\n            messages,\n            model,\n            temperature,\n            seed,\n            max_tokens,\n        )\n    async def create_completion_async(\n            self,\n            messages: List[Dict[str, str]],\n            model: str | None = None,\n            temperature: float = config.temperature,\n            seed: int = config.seed,\n            max_tokens: int = config.max_tokens,\n    ) -> Tuple[str, Dict[str, int]]:\n        if model is None:\n            model = self.llm_model"
        },
        {
            "comment": "This code snippet is responsible for creating a function that generates a response asynchronously, with support for retrying if an API error, rate limit error, or timeout occurs. The function sends a request to the OpenAI API using Azure client for chat completion and takes parameters such as messages, model, temperature, seed, and max_tokens. If debug mode is enabled, it logs information about the completion being created; otherwise, it simply logs that the completion request is being made.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":372-399",
            "content": "        if config.debug_mode:\n            logger.debug(\n                f\"Creating chat completion with model {model}, temperature {temperature}, max_tokens {max_tokens}\")\n        else:\n            logger.write(f\"Requesting {model} completion...\")\n        @backoff.on_exception(\n            backoff.constant,\n            (\n                    APIError,\n                    RateLimitError,\n                    APITimeoutError),\n            max_tries=self.retries,\n            interval=10,\n        )\n        async def _generate_response_with_retry_async(\n                messages: List[Dict[str, str]],\n                model: str,\n                temperature: float,\n                seed: int = None,\n                max_tokens: int = 512,\n        ) -> Tuple[str, Dict[str, int]]:\n            \"\"\"Send a request to the OpenAI API.\"\"\"\n            if self.provider_cfg[PROVIDER_SETTING_IS_AZURE]:\n                response = await asyncio.to_thread(\n                    self.client.chat.completions.create,\n                    model=model,"
        },
        {
            "comment": "This code retrieves a response from OpenAI's chat completions API, based on the specified model, messages, temperature, seed, and maximum tokens. If the response is None, it tries again and logs an error message. The function then extracts the content of the first message in the response, along with usage information such as prompt, completion, and total tokens, and finally writes a log message indicating the completion of the operation.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":400-428",
            "content": "                    messages=messages,\n                    temperature=temperature,\n                    seed=seed,\n                    max_tokens=max_tokens,\n                )\n            else:\n                response = await asyncio.to_thread(\n                    self.client.chat.completions.create,\n                    model=model,\n                    messages=messages,\n                    temperature=temperature,\n                    seed=seed,\n                    max_tokens=max_tokens,\n                )\n            if response is None:\n                logger.error(\"Failed to get a response from OpenAI. Try again.\")\n                logger.double_check()\n            message = response.choices[0].message.content\n            info = {\n                \"prompt_tokens\": response.usage.prompt_tokens,\n                \"completion_tokens\": response.usage.completion_tokens,\n                \"total_tokens\": response.usage.total_tokens,\n                \"system_fingerprint\": response.system_fingerprint,\n            }\n            logger.write(f'Response received from {model}.')"
        },
        {
            "comment": "This code snippet is part of a provider class for OpenAI's services. It defines a method to generate a response using the OpenAI API and counts the number of tokens used by a list of messages based on the model provided. The code uses the Tiktoken encoding for tokenization, and handles exceptions for unsupported models like GPT-4V.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":430-460",
            "content": "            return message, info\n        return await _generate_response_with_retry_async(\n            messages,\n            model,\n            temperature,\n            seed,\n            max_tokens,\n        )\n    def num_tokens_from_messages(self, messages, model):\n        \"\"\"Return the number of tokens used by a list of messages.\n        Borrowed from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n        \"\"\"\n        try:\n            encoding = tiktoken.encoding_for_model(model)\n        except KeyError:\n            logger.debug(\"Warning: model not found. Using cl100k_base encoding.\")\n            encoding = tiktoken.get_encoding(\"cl100k_base\")\n        if model in {\n            \"gpt-4-1106-vision-preview\",\n        }:\n            raise ValueError(\"We don't support counting tokens of GPT-4V yet.\")\n        if model in {\n            \"gpt-3.5-turbo-0613\",\n            \"gpt-3.5-turbo-16k-0613\",\n            \"gpt-4-0314\",\n            \"gpt-4-32k-0314\",\n            \"gpt-4-0613\","
        },
        {
            "comment": "The code calculates the number of tokens required for a given message in an OpenAI ChatGPT model based on the model type. It handles models like \"gpt-4-32k-0613\" and \"gpt-3.5-turbo-0301\". The tokens calculation includes the number of messages, their content, name (if present), and prime for each reply. If the model is not supported, it raises a NotImplementedError with a link to information on how messages are converted to tokens.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":461-484",
            "content": "            \"gpt-4-32k-0613\",\n            \"gpt-4-1106-preview\",\n        }:\n            tokens_per_message = 3\n            tokens_per_name = 1\n        elif model == \"gpt-3.5-turbo-0301\":\n            tokens_per_message = (\n                4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n            )\n            tokens_per_name = -1  # if there's a name, the role is omitted\n        else:\n            raise NotImplementedError(\n                f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n            )\n        num_tokens = 0\n        for message in messages:\n            num_tokens += tokens_per_message\n            for key, value in message.items():\n                num_tokens += len(encoding.encode(value))\n                if key == \"name\":\n                    num_tokens += tokens_per_name\n        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>"
        },
        {
            "comment": "The code defines a function `assemble_prompt_tripartite` which assembles a tripartite prompt consisting of system, user messages, and an image introduction. It uses regular expression to parse the template string into paragraphs, filters empty or whitespace-only paragraphs, and sets the system content as the first non-empty paragraph. The function returns the assembled prompt in list format.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":486-515",
            "content": "        return num_tokens\n    def _get_azure_deployment_id_for_model(self, model_label) -> list:\n        return self.provider_cfg[PROVIDER_SETTING_DEPLOYMENT_MAP][model_label]\n    def assemble_prompt_tripartite(self, template_str: str = None, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        A tripartite prompt is a message with the following structure:\n        <system message>\n        <user message part 1 before image introduction>\n        <image introduction>\n        <user message part 2 after image introduction>\n        \"\"\"\n        pattern = re.compile(r\"(.+?)(?=\\n\\n|$)\", re.DOTALL)\n        paragraphs = re.findall(pattern, template_str)\n        filtered_paragraphs = [p for p in paragraphs if p.strip() != '']\n        system_content = filtered_paragraphs[0]  # the system content defaults to the first paragraph of the template\n        system_message = {\n            \"role\": \"system\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": f\"{system_content}\""
        },
        {
            "comment": "This code segments \"paragraphs\" in a document, identifying those containing the constant \"IMAGES_INPUT_TAG\". It then divides the filtered paragraphs into two sections: before and after the image introduction paragraph. The code assembles user messages part 1 by excluding any paragraphs with placeholders like \"<$...$>\".",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":516-541",
            "content": "                }\n            ]\n        }\n        # segmenting \"paragraphs\"\n        image_introduction_paragraph_index = None\n        image_introduction_paragraph = None\n        for i, paragraph in enumerate(filtered_paragraphs):\n            if constants.IMAGES_INPUT_TAG in paragraph:\n                image_introduction_paragraph_index = i\n                image_introduction_paragraph = paragraph\n                break\n        user_messages_part1_paragraphs = filtered_paragraphs[1:image_introduction_paragraph_index]\n        user_messages_part2_paragraphs = filtered_paragraphs[image_introduction_paragraph_index + 1:]\n        # assemble user messages part 1\n        user_messages_part1_contents = []\n        for paragraph in user_messages_part1_paragraphs:\n            search_placeholder_pattern = re.compile(r\"<\\$[^\\$]+\\$>\")\n            placeholder = re.search(search_placeholder_pattern, paragraph)\n            if not placeholder:\n                user_messages_part1_contents.append(paragraph)\n            else:\n                placeholder = placeholder.group()"
        },
        {
            "comment": "Checks if the input paragraph matches the placeholder and appends modified content to user_messages_part1_contents based on input type. If unexpected input type is encountered, raises ValueError. Finally, joins contents with newlines and assigns to user_messages_part1.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":542-559",
            "content": "                placeholder_name = placeholder.replace(\"<$\", \"\").replace(\"$>\", \"\")\n                paragraph_input = params.get(placeholder_name, None)\n                if paragraph_input is None or paragraph_input == \"\" or paragraph_input == []:\n                    continue\n                else:\n                    if isinstance(paragraph_input, str):\n                        paragraph_content = paragraph.replace(placeholder, paragraph_input)\n                        user_messages_part1_contents.append(paragraph_content)\n                    elif isinstance(paragraph_input, list):\n                        paragraph_content = paragraph.replace(placeholder, json.dumps(paragraph_input))\n                        user_messages_part1_contents.append(paragraph_content)\n                    else:\n                        raise ValueError(f\"Unexpected input type: {type(paragraph_input)}\")\n        user_messages_part1_content = \"\\n\\n\".join(user_messages_part1_contents)\n        user_messages_part1 = {\n            \"role\": \"user\","
        },
        {
            "comment": "The code assembles image introduction messages based on input paragraphs. If the paragraph_input is null, empty or an empty list, it creates an empty image_introduction_messages array. Otherwise, it removes the IMAGES_INPUT_TAG from the image_introduction_paragraph and adds a message to the image_introduction_messages array, with role as 'user' and content as the text from paragraph_content_pre. Then it iterates over each item in the paragraph_input and assigns the value of IMAGE_INTRO_TAG_NAME to the variable introduction, if not null.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":560-589",
            "content": "            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": f\"{user_messages_part1_content}\"\n                }\n            ]\n        }\n        # assemble image introduction messages\n        image_introduction_messages = []\n        paragraph_input = params.get(constants.IMAGES_INPUT_TAG_NAME, None)\n        if paragraph_input is None or paragraph_input == \"\" or paragraph_input == []:\n            image_introduction_messages = []\n        else:\n            paragraph_content_pre = image_introduction_paragraph.replace(constants.IMAGES_INPUT_TAG, \"\")\n            message = {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"text\",\n                        \"text\": f\"{paragraph_content_pre}\"\n                    }\n                ]\n            }\n            image_introduction_messages.append(message)\n            for item in paragraph_input:\n                introduction = item.get(constants.IMAGE_INTRO_TAG_NAME, None)"
        },
        {
            "comment": "This code is responsible for generating a message containing user input, an image, and additional information such as the image path, assistant, and resolution. It checks if the introduction is not empty and encodes the image path to base64 format before adding it to the message content.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":590-613",
            "content": "                path = item.get(constants.IMAGE_PATH_TAG_NAME, None)\n                assistant = item.get(constants.IMAGE_ASSISTANT_TAG_NAME, None)\n                resolution = item.get(constants.IMAGE_RESOLUTION_TAG_NAME, None)\n                if introduction is not None and introduction != \"\":\n                    message = {\n                        \"role\": \"user\",\n                        \"content\": [\n                            {\n                                \"type\": \"text\",\n                                \"text\": f\"{introduction}\"\n                            }\n                        ]\n                    }\n                    if path is not None and path != \"\":\n                        encoded_images = encode_data_to_base64_path(path)\n                        for encoded_image in encoded_images:\n                            msg_content = {\n                                    \"type\": \"image_url\",\n                                    \"image_url\":\n                                        {\n                                            \"url\": f\"{encoded_image}\""
        },
        {
            "comment": "The code is defining a function to assemble image introduction messages and user messages part 2. It creates a message object with the role \"user\" or \"assistant\" and appends it to the respective lists based on if there is any text in the variables \"user_messages_part1\", \"image\", and \"assistant\". The code also defines a pattern for search placeholders using regular expressions (regex) with the variable \"search_placeholder_pattern\".",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":614-639",
            "content": "                                        }\n                                }\n                            if resolution is not None and resolution != \"\":\n                                msg_content[\"image_url\"][\"detail\"] = resolution\n                            message[\"content\"].append(msg_content)\n                    image_introduction_messages.append(message)\n                if assistant is not None and assistant != \"\":\n                    message = {\n                        \"role\": \"assistant\",\n                        \"content\": [\n                            {\n                                \"type\": \"text\",\n                                \"text\": f\"{assistant}\"\n                            }\n                        ]\n                    }\n                    image_introduction_messages.append(message)\n        # assemble user messages part 2\n        user_messages_part2_contents = []\n        for paragraph in user_messages_part2_paragraphs:\n            search_placeholder_pattern = re.compile(r\"<\\$[^\\$]+\\$>\")"
        },
        {
            "comment": "This code is searching for placeholders in the paragraph using a regular expression. If a placeholder is found, it extracts its name and checks if there's a corresponding input value from params. If the input exists, it replaces the placeholder with the input content (as a string or formatted as JSON if input is a list). Otherwise, it continues to the next paragraph without modifying this one.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":641-658",
            "content": "            placeholder = re.search(search_placeholder_pattern, paragraph)\n            if not placeholder:\n                user_messages_part2_contents.append(paragraph)\n            else:\n                placeholder = placeholder.group()\n                placeholder_name = placeholder.replace(\"<$\", \"\").replace(\"$>\", \"\")\n                paragraph_input = params.get(placeholder_name, None)\n                if paragraph_input is None or paragraph_input == \"\" or paragraph_input == []:\n                    continue\n                else:\n                    if isinstance(paragraph_input, str):\n                        paragraph_content = paragraph.replace(placeholder, paragraph_input)\n                        user_messages_part2_contents.append(paragraph_content)\n                    elif isinstance(paragraph_input, list):\n                        paragraph_content = paragraph.replace(placeholder, json.dumps(paragraph_input))\n                        user_messages_part2_contents.append(paragraph_content)\n                    else:"
        },
        {
            "comment": "This code defines functions for assembling prompts. It checks the message construction mode and returns a list of dictionaries representing messages in a prompt. If the mode is tripartite, it calls another function to generate the prompt. However, the \"assemble_prompt_paragraph\" and \"assemble_prompt\" functions are not implemented yet and raise an error when called.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":659-681",
            "content": "                        raise ValueError(f\"Unexpected input type: {type(paragraph_input)}\")\n        user_messages_part2_content = \"\\n\\n\".join(user_messages_part2_contents)\n        user_messages_part2 = {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": f\"{user_messages_part2_content}\"\n                }\n            ]\n        }\n        return [system_message] + [user_messages_part1] + image_introduction_messages + [user_messages_part2]\n    def assemble_prompt_paragraph(self, template_str: str = None, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n        raise NotImplementedError(\"This method is not implemented yet.\")\n    def assemble_prompt(self, template_str: str = None, params: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n        if config.DEFAULT_MESSAGE_CONSTRUCTION_MODE == constants.MESSAGE_CONSTRUCTION_MODE_TRIPART:\n            return self.assemble_prompt_tripartite(template_str=template_str, params=params)"
        },
        {
            "comment": "The code snippet demonstrates a function to encode images and their paths using base64 encoding. It first checks the default message construction mode, then encodes image binary data from a file, logs an image hash with its path, decodes base64-encoded images, and encodes data to base64 format for various types of input (strings, images, or numpy arrays).",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":682-713",
            "content": "        elif config.DEFAULT_MESSAGE_CONSTRUCTION_MODE == constants.MESSAGE_CONSTRUCTION_MODE_PARAGRAPH:\n            return self.assemble_prompt_paragraph(template_str=template_str, params=params)\ndef encode_image_path(image_path):\n    with open(image_path, \"rb\") as image_file:\n        encoded_image = encode_image_binary(image_file.read(), image_path)\n        return encoded_image\ndef encode_image_binary(image_binary, image_path=None):\n    encoded_image = encode_base64(image_binary)\n    if image_path is None:\n        image_path = '<$bin_placeholder$>'\n    logger.debug(f'|>. img_hash {hash_text_sha256(encoded_image)}, path {image_path} .<|')\n    return encoded_image\ndef decode_image(base64_encoded_image):\n    return decode_base64(base64_encoded_image)\ndef encode_data_to_base64_path(data: Any) -> List[str]:\n    encoded_images = []\n    if isinstance(data, (str, Image.Image, np.ndarray, bytes)):\n        data = [data]\n    for item in data:\n        if isinstance(item, str):\n            if os.path.exists(assemble_project_path(item)):"
        },
        {
            "comment": "This code handles various image formats and converts them into JPEG format. It checks the type of item, whether it is a file path, bytes, PIL Image, or numpy ndarray. Depending on the type, it performs necessary conversions to make the image compatible with the JPEG format. The image is then encoded as base64 and appended to the list of encoded images.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":714-737",
            "content": "                path = assemble_project_path(item)\n                encoded_image = encode_image_path(path)\n                image_type = path.split(\".\")[-1].lower()\n                encoded_image = f\"data:image/{image_type};base64,{encoded_image}\"\n                encoded_images.append(encoded_image)\n            else:\n                encoded_images.append(item)\n            continue\n        elif isinstance(item, bytes):  # mss grab bytes\n            image = Image.frombytes('RGB', item.size, item.bgra, 'raw', 'BGRX')\n            buffered = io.BytesIO()\n            image.save(buffered, format=\"JPEG\")\n        elif isinstance(item, Image.Image):  # PIL image\n            buffered = io.BytesIO()\n            item.save(buffered, format=\"JPEG\")\n        elif isinstance(item, np.ndarray):  # cv2 image array\n            item = cv2.cvtColor(item, cv2.COLOR_BGR2RGB)  # convert to RGB\n            image = Image.fromarray(item)\n            buffered = io.BytesIO()\n            image.save(buffered, format=\"JPEG\")\n        encoded_image = encode_image_binary(buffered.getvalue())"
        },
        {
            "comment": "This code snippet encodes an image and appends it to the encoded_images list before returning the final list.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/cradle/provider/openai.py\":738-741",
            "content": "        encoded_image = f\"data:image/jpeg;base64,{encoded_image}\"\n        encoded_images.append(encoded_image)\n    return encoded_images"
        }
    ]
}