{
    "summary": "This code imports modules, manages game actions and AI providers, records data, configures inputs and outputs, annotates frames, logs info, classifies, checks errors, updates descriptions, assigns tasks to skills, handles image data, and summarizes from memory.",
    "details": [
        {
            "comment": "The code imports various modules and classes from different packages related to game management, agent control, logging, and memory. It also includes functions and skills for atomic and composite actions such as trading, buying, mapping, moving, and combat.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":0-24",
            "content": "import os\nimport time\nimport copy\nimport argparse\nfrom groundingdino.util.inference import load_image\nfrom cradle.config import Config\nfrom cradle.gameio.game_manager import GameManager\nfrom cradle.log import Logger\nfrom cradle.agent import Agent\nfrom cradle.planner.planner import Planner\nfrom cradle.memory import LocalMemory\nfrom cradle.provider.openai import OpenAIProvider\nfrom cradle.provider import GdProvider\nfrom cradle.gameio.io_env import IOEnvironment\nfrom cradle.gameio.lifecycle.ui_control import switch_to_game, IconReplacer\nfrom cradle.gameio.video.VideoRecorder import VideoRecorder\nfrom cradle.gameio.video.VideoFrameExtractor import VideoFrameExtractor\nfrom cradle.gameio.atomic_skills.trade_utils import __all__ as trade_skills\nfrom cradle.gameio.atomic_skills.buy import __all__ as buy_skills\nfrom cradle.gameio.atomic_skills.map import __all__ as map_skills\nfrom cradle.gameio.atomic_skills.move import __all__ as move_skills\nfrom cradle.gameio.atomic_skills.combat import __all__ as combat_skills\nfrom cradle.gameio.composite_skills.auto_shoot import __all__ as auto_shoot_skills"
        },
        {
            "comment": "This code initializes a pipeline loop for a Cradle prototype. It sets up an LLM provider, Gd detector, video frame extractor, and icon replacer. It also creates a planner and a local memory, configuring various parameters and options.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":25-52",
            "content": "from cradle.gameio.composite_skills.follow import __all__ as follow_skills\nfrom cradle import constants\nconfig = Config()\nlogger = Logger()\nio_env = IOEnvironment()\ndef trigger_pipeline_loop(llm_provider_config_path, planner_params, task_description, skill_library, use_success_detection = False, use_self_reflection = False, use_information_summary = False):\n    llm_provider = OpenAIProvider()\n    llm_provider.init_provider(llm_provider_config_path)\n    gd_detector = GdProvider()\n    frame_extractor = VideoFrameExtractor()\n    icon_replacer = IconReplacer()\n    planner = Planner(llm_provider=llm_provider,\n                      planner_params=planner_params,\n                      frame_extractor=frame_extractor,\n                      icon_replacer=icon_replacer,\n                      object_detector=gd_detector,\n                      use_self_reflection=use_self_reflection,\n                      use_information_summary=use_information_summary)\n    memory = LocalMemory(memory_path=config.work_dir,\n                         max_recent_steps=config.max_recent_steps)"
        },
        {
            "comment": "The code loads game environment, registers available skills, retrieves relevant skills based on task description and number of skills needed, starts video recording, captures initial screen shot, adds it to recent history, waits for 2 seconds, and then records the end frame id.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":53-81",
            "content": "    memory.load(config.memory_load_path)\n    gm = GameManager(env_name = config.env_name,\n                     embedding_provider = llm_provider)\n    img_prompt_decision_making = planner.decision_making_.input_map[\"image_introduction\"]\n    if config.skill_retrieval:\n        gm.register_available_skills(skill_library)\n        skill_library = gm.retrieve_skills(query_task = task_description, skill_num = config.skill_num, screen_type = constants.GENERAL_GAME_INTERFACE)\n    skill_library = gm.get_skill_information(skill_library)\n    switch_to_game()\n    videocapture=VideoRecorder(os.path.join(config.work_dir, 'video.mp4'))\n    videocapture.start_capture()\n    start_frame_id = videocapture.get_current_frame_id()\n    cur_screen_shot_path, _ = gm.capture_screen()\n    memory.add_recent_history(\"image\", cur_screen_shot_path)\n    success = False\n    pre_action = \"\"\n    pre_screen_classification = \"\"\n    pre_decision_making_reasoning = \"\"\n    pre_self_reflection_reasoning = \"\"\n    time.sleep(2)\n    end_frame_id = videocapture.get_current_frame_id()"
        },
        {
            "comment": "Code snippet from Cradle/prototype_runner.py:82-105 is related to the Gather Information task in an AI application. It pauses the game, gathers information preparation details like frame IDs and video clip path, retrieves task guidance and configurations, and prepares the gather_information module with frame extraction and icon replacement enabled. This section seems to be involved in data collection and task configuration for the Gather Information function.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":82-105",
            "content": "    gm.pause_game()\n    while not success:\n        try:\n            # Gather information preparation\n            logger.write(f'Gather Information Start Frame ID: {start_frame_id}, End Frame ID: {end_frame_id}')\n            input = planner.gather_information_.input_map\n            text_input = planner.gather_information_.text_input_map\n            video_clip_path = videocapture.get_video(start_frame_id,end_frame_id)\n            task_description = memory.get_task_guidance(use_last=False)\n            get_text_image_introduction = [\n                {\n                    \"introduction\": input[\"image_introduction\"][-1][\"introduction\"],\n                    \"path\": memory.get_recent_history(\"image\", k=1)[0],\n                    \"assistant\": input[\"image_introduction\"][-1][\"assistant\"]\n                }\n            ]\n            # Configure the gather_information module\n            gather_information_configurations = {\n                \"frame_extractor\": True, # extract text from the video clip\n                \"icon_replacer\": True,"
        },
        {
            "comment": "This code is configuring and passing input parameters to the information gathering function. It sets up configurations for object detection, screenshot description, and image introduction. Then it modifies the general input and text input for the get_text module. Finally, it calls the gather_information function with the prepared inputs and stores the gathered information in the gathered_information_JSON variable.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":106-126",
            "content": "                \"llm_description\": True, # get the description of the current screenshot\n                \"object_detector\": True\n            }\n            input[\"gather_information_configurations\"] = gather_information_configurations\n            # Modify the general input for gather_information here\n            image_introduction=[get_text_image_introduction[-1]]\n            input[\"task_description\"] = task_description\n            input[\"video_clip_path\"] = video_clip_path\n            input[\"image_introduction\"] = image_introduction\n            # Modify the input for get_text module in gather_information here\n            text_input[\"image_introduction\"] = get_text_image_introduction\n            input[\"text_input\"] = text_input\n            # >> Calling INFORMATION GATHERING\n            logger.write(f'>> Calling INFORMATION GATHERING')\n            data = planner.gather_information(input=input)\n            # Any information from the gathered_information_JSON\n            gathered_information_JSON=data['res_dict']['gathered_information_JSON']"
        },
        {
            "comment": "This code checks if the gathered_information_JSON exists, if not it logs a warning. If it does exist, it sorts the data by timestamp and extracts specific types of data (dialogue, task guidance, generated actions, classification reasons) from it. It then checks if 'LAST_TASK_GUIDANCE' is present in the response keys.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":128-143",
            "content": "            if gathered_information_JSON is not None:\n                gathered_information=gathered_information_JSON.data_structure\n            else:\n                logger.warn(\"NO data_structure in gathered_information_JSON\")\n                gathered_information = dict()\n            # Sort the gathered_information by timestamp\n            gathered_information = dict(sorted(gathered_information.items(), key=lambda item: item[0]))\n            all_dialogue = gathered_information_JSON.search_type_across_all_indices(constants.DIALOGUE)\n            all_task_guidance = gathered_information_JSON.search_type_across_all_indices(constants.TASK_GUIDANCE)\n            all_generated_actions = gathered_information_JSON.search_type_across_all_indices(constants.ACTION_GUIDANCE)\n            classification_reasons = gathered_information_JSON.search_type_across_all_indices(constants.GATHER_TEXT_REASONING)\n            response_keys = data['res_dict'].keys()\n            if constants.LAST_TASK_GUIDANCE in response_keys:"
        },
        {
            "comment": "The code extracts task guidance, image description, and screen classification from a response dictionary. If any of these values are missing, it logs a warning and sets the value to a default string. The first character of last_task_horizon is used as the boolean value.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":144-162",
            "content": "                last_task_guidance = data['res_dict'][constants.LAST_TASK_GUIDANCE]\n                if constants.LAST_TASK_HORIZON in response_keys:\n                    long_horizon = bool(int(data['res_dict'][constants.LAST_TASK_HORIZON][0])) # Only first character is relevant\n                else:\n                    long_horizon = False\n            else:\n                logger.warn(f\"No {constants.LAST_TASK_GUIDANCE} in response.\")\n                last_task_guidance = \"\"\n                long_horizon = False\n            if constants.IMAGE_DESCRIPTION in response_keys:\n                image_description=data['res_dict'][constants.IMAGE_DESCRIPTION]\n                if constants.SCREEN_CLASSIFICATION in response_keys:\n                    screen_classification=data['res_dict'][constants.SCREEN_CLASSIFICATION]\n                else:\n                    screen_classification=\"None\"\n            else:\n                logger.warn(f\"No {constants.IMAGE_DESCRIPTION} in response.\")\n                image_description=\"No description\""
        },
        {
            "comment": "This code checks the screen classification, retrieves target object information and reasoning, loads an image from the current screenshot, and extracts bounding boxes, logits, and phrases from the response dictionary. It also creates a path for storing the bounding box image.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":163-183",
            "content": "                screen_classification=\"None\"\n            # Return to pause if screen type changed\n            if screen_classification.lower() == constants.GENERAL_GAME_INTERFACE:\n                gm.pause_game(screen_classification.lower())\n            if constants.TARGET_OBJECT_NAME in response_keys:\n                target_object_name=data['res_dict'][constants.TARGET_OBJECT_NAME]\n                object_name_reasoning=data['res_dict'][constants.GATHER_INFO_REASONING]\n            else:\n                logger.write(\"> No target object\")\n                target_object_name = \"\"\n                object_name_reasoning=\"\"\n            if \"boxes\" in response_keys:\n                image_source, image = load_image(cur_screen_shot_path)\n                boxes = data['res_dict'][\"boxes\"]\n                logits = data['res_dict'][\"logits\"]\n                phrases = data['res_dict'][\"phrases\"]\n                directory, filename = os.path.split(cur_screen_shot_path)\n                bb_image_path = os.path.join(directory, \"bb_\"+filename)"
        },
        {
            "comment": "The code saves annotated frames, adds screenshots with bounding boxes to working memory if any, and logs image description, object name, reasoning, screen classification, dialogue, and gathered information.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":184-200",
            "content": "                gd_detector.save_annotate_frame(image_source, boxes, logits, phrases, target_object_name.title(), bb_image_path)\n                if boxes is not None and boxes.numel() != 0:\n                    # Add the screenshot with bounding boxes into working memory\n                    memory.add_recent_history(key=constants.AUGMENTED_IMAGES_MEM_BUCKET, info=bb_image_path)\n                else:\n                    memory.add_recent_history(key=constants.AUGMENTED_IMAGES_MEM_BUCKET, info=constants.NO_IMAGE)\n            else:\n                memory.add_recent_history(key=constants.AUGMENTED_IMAGES_MEM_BUCKET, info=constants.NO_IMAGE)\n            logger.write(f'Image Description: {image_description}')\n            logger.write(f'Object Name: {target_object_name}')\n            logger.write(f'Reasoning: {object_name_reasoning}')\n            logger.write(f'Screen Classification: {screen_classification}')\n            logger.write(f'Dialogue: {all_dialogue}')\n            logger.write(f'Gathered Information: {gathered_information}')"
        },
        {
            "comment": "This code writes classification reasons, task guidance, and long horizon to the logger. It then checks if self-reflection is enabled and gets action frames from a video capture object based on certain conditions.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":201-217",
            "content": "            logger.write(f'Classification Reasons: {classification_reasons}')\n            logger.write(f'All Task Guidance: {all_task_guidance}')\n            logger.write(f'Last Task Guidance: {last_task_guidance}')\n            logger.write(f'Long Horizon: {long_horizon}')\n            logger.write(f'Generated Actions: {all_generated_actions}')\n            if use_self_reflection and start_frame_id > -1:\n                input = planner.self_reflection_.input_map\n                action_frames = []\n                video_frames = videocapture.get_frames(start_frame_id,end_frame_id)\n                if len(video_frames) <= config.max_images_in_self_reflection * config.duplicate_frames + 1:\n                    action_frames = [frame[1] for frame in video_frames[1::config.duplicate_frames]]\n                else:\n                    for i in range(config.max_images_in_self_reflection):\n                        step = len(video_frames) // config.max_images_in_self_reflection * i + 1\n                        action_frames.append(video_frames[step][1])"
        },
        {
            "comment": "This code section is setting up the input for an AI model. It includes an image_introduction, task_description, skill_library, and previous_reasoning. The pre_action (if exists) is also converted into a skill and inputted as previous_action. The action_code for this skill is obtained from the skill_library if it exists, otherwise, the action code info is inputted.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":219-238",
            "content": "                image_introduction = [\n                    {\n                        \"introduction\": \"Here are the sequential frames of the character executing the last action.\",\n                        \"path\": action_frames,\n                        \"assistant\": \"\",\n                        \"resolution\": \"low\"\n                }]\n                input[\"image_introduction\"] = image_introduction\n                input[\"task_description\"] = task_description\n                input['skill_library'] = skill_library\n                input[\"previous_reasoning\"] = pre_decision_making_reasoning\n                if pre_action:\n                    pre_action_name, pre_action_params = gm.skill_registry.convert_expression_to_skill(pre_action)\n                    # only input the pre_action name\n                    input[\"previous_action\"] = pre_action_name\n                    action_code, action_code_info = gm.get_skill_library_in_code(pre_action_name)\n                    input['action_code'] = action_code if action_code is not None else action_code_info"
        },
        {
            "comment": "Code snippet sets the previous action and action code to empty strings if they don't exist, handles executing action errors, calls self-reflection using a planner function, retrieves reasoning from the returned data, adds recent history to memory about self-reflection reasoning, and logs the reasoning.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":239-261",
            "content": "                else:\n                    input[\"previous_action\"] = \"\"\n                    input['action_code'] = \"\"\n                if exec_info[\"errors\"]:\n                    input['executing_action_error']  = exec_info[\"errors_info\"]\n                else:\n                    input['executing_action_error']  = \"\"\n                # >> Calling SELF REFLECTION\n                logger.write(f'>> Calling SELF REFLECTION')\n                reflection_data = planner.self_reflection(input = input)\n                if 'reasoning' in reflection_data['res_dict'].keys():\n                    self_reflection_reasoning = reflection_data['res_dict']['reasoning']\n                else:\n                    self_reflection_reasoning = \"\"\n                pre_self_reflection_reasoning = self_reflection_reasoning\n                memory.add_recent_history(\"self_reflection_reasoning\", self_reflection_reasoning)\n                logger.write(f'Self-reflection reason: {self_reflection_reasoning}')\n            if last_task_guidance:"
        },
        {
            "comment": "This code block updates the task description from previous guidance, adds it to memory, and writes the current task description to a logger. It then retrieves skills based on the task description using skill_retrieval configuration. The skill library is populated with skill information and the video capture's frame buffer is cleared for decision making preparation.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":262-282",
            "content": "                task_description = last_task_guidance\n                memory.add_task_guidance(last_task_guidance, long_horizon)\n            logger.write(f'Current Task Guidance: {task_description}')\n            if config.skill_retrieval:\n                for extracted_skills in all_generated_actions:\n                    extracted_skills=extracted_skills['values']\n                    for extracted_skill in extracted_skills:\n                        gm.add_new_skill(skill_code=extracted_skill['code'])\n                skill_library = gm.retrieve_skills(query_task = task_description, skill_num = config.skill_num, screen_type = screen_classification.lower())\n                logger.write(f'skill_library: {skill_library}')\n                skill_library = gm.get_skill_information(skill_library)\n            videocapture.clear_frame_buffer()\n            # Decision making preparation\n            input = copy.deepcopy(planner.decision_making_.input_map)\n            number_of_execute_skills = input[\"number_of_execute_skills\"]"
        },
        {
            "comment": "The code is checking if any actions or reasoning occurred before the current action and storing them in the input dictionary. It also retrieves a summarization from memory, checks for existing 'boxes' in the response_keys, and populates 'few_shots' if none exist. Finally, it gets recent image data from memory and handles possible augmented images as well.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":284-302",
            "content": "            if pre_action:\n                input[\"previous_action\"] = memory.get_recent_history(\"action\", k=1)[-1]\n                input[\"previous_reasoning\"] = memory.get_recent_history(\"decision_making_reasoning\", k=1)[-1]\n            if pre_self_reflection_reasoning:\n                input[\"previous_self_reflection_reasoning\"] = memory.get_recent_history(\"self_reflection_reasoning\", k=1)[-1]\n            input['skill_library'] = skill_library\n            input['info_summary'] = memory.get_summarization()\n            if not \"boxes\" in response_keys:\n                input['few_shots'] = []\n            else:\n                if boxes is None or boxes.numel() == 0:\n                    input['few_shots'] = []\n            # @TODO Temporary solution with fake augmented entries if no bounding box exists. Ideally it should read images, then check for possible augmentation.\n            image_memory = memory.get_recent_history(\"image\", k=config.decision_making_image_num)\n            augmented_image_memory = memory.get_recent_history(constants.AUGMENTED_IMAGES_MEM_BUCKET, k=config.decision_making_image_num)"
        },
        {
            "comment": "This code initializes an empty list \"image_introduction\". It then iterates through the \"augmented_image_memory\" and \"img_prompt_decision_making\" lists in reverse order, adding dictionaries to \"image_introduction\" based on conditions. These dictionaries contain information related to image introductions, paths, and assistants. Finally, it assigns the \"image_introduction\" list and a \"task_description\" variable to the \"input\" dictionary. The code also checks if \"constants.MINIMAP_INFORMATION\" is in \"response_keys\".",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":304-325",
            "content": "            image_introduction = []\n            for i in range(len(image_memory), 0, -1):\n                if augmented_image_memory[-i] != constants.NO_IMAGE:\n                    image_introduction.append(\n                        {\n                            \"introduction\": img_prompt_decision_making[-i][\"introduction\"],\n                            \"path\":augmented_image_memory[-i],\n                            \"assistant\": img_prompt_decision_making[-i][\"assistant\"]\n                        })\n                else:\n                    image_introduction.append(\n                        {\n                            \"introduction\": img_prompt_decision_making[-i][\"introduction\"],\n                            \"path\":image_memory[-i],\n                            \"assistant\": img_prompt_decision_making[-i][\"assistant\"]\n                        })\n            input[\"image_introduction\"] = image_introduction\n            input[\"task_description\"] = task_description\n            # Minimap info tracking\n            if constants.MINIMAP_INFORMATION in response_keys:"
        },
        {
            "comment": "The code retrieves minimap information and formats it for logging. It then proceeds to get skill steps from the decision-making process, filters out any empty or 'nop' actions, and logs the final list of non-empty actions (R).",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":326-348",
            "content": "                minimap_information = data[\"res_dict\"][constants.MINIMAP_INFORMATION]\n                logger.write(f\"{constants.MINIMAP_INFORMATION}: {minimap_information}\")\n                minimap_info_str = \"\"\n                for key, value in minimap_information.items():\n                    if value:\n                        for index, item in enumerate(value):\n                            minimap_info_str = minimap_info_str + key + ' ' + str(index) + ': angle '  + str(int(item['theta'])) + ' degree' + '\\n'\n                minimap_info_str = minimap_info_str.rstrip('\\n')\n                logger.write(f'minimap_info_str: {minimap_info_str}')\n                input[constants.MINIMAP_INFORMATION] = minimap_info_str\n            data = planner.decision_making(input = input)\n            skill_steps = data['res_dict']['actions']\n            if skill_steps is None:\n                skill_steps = []\n            logger.write(f'R: {skill_steps}')\n            # Filter nop actions in list\n            skill_steps = [ i for i in skill_steps if i != '']"
        },
        {
            "comment": "This code segment checks if skill_steps list is empty, then assigns a non-empty value. It limits the number of execute skills based on number_of_execute_skills. It logs Skill Steps, unpauses the game and performs actions using General Game Interface (possibly needs renaming) under specific conditions. It captures pre and post screen shots and pauses the game upon completion. The code also notes any execution errors and provides a list of successfully executed skills in exec_info.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":349-370",
            "content": "            if len(skill_steps) == 0:\n                skill_steps = ['']\n            skill_steps = skill_steps[:number_of_execute_skills]\n            logger.write(f'Skill Steps: {skill_steps}')\n            gm.unpause_game()\n            # @TODO: Rename GENERAL_GAME_INTERFACE\n            if pre_screen_classification.lower() == constants.GENERAL_GAME_INTERFACE and (screen_classification.lower() == constants.MAP_INTERFACE or screen_classification.lower() == constants.SATCHEL_INTERFACE) and pre_action:\n                exec_info = gm.execute_actions([pre_action])\n            start_frame_id = videocapture.get_current_frame_id()\n            exec_info = gm.execute_actions(skill_steps)\n            cur_screen_shot_path, _ = gm.capture_screen()\n            end_frame_id = videocapture.get_current_frame_id()\n            gm.pause_game(screen_classification.lower())\n            # exec_info also has the list of successfully executed skills. skill_steps is the full list, which may differ if there were execution errors."
        },
        {
            "comment": "Code retrieves the last skill executed, any previous decision-making reasoning, and updates memory with recent history. It checks if there's a need for an information summary call based on recent steps and event count, then logs messages.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":371-388",
            "content": "            pre_action = exec_info[\"last_skill\"]\n            pre_decision_making_reasoning = ''\n            if 'res_dict' in data.keys() and 'reasoning' in data['res_dict'].keys():\n                pre_decision_making_reasoning = data['res_dict']['reasoning']\n            pre_screen_classification = screen_classification\n            memory.add_recent_history(\"action\", pre_action)\n            memory.add_recent_history(\"decision_making_reasoning\", pre_decision_making_reasoning)\n            # For such cases with no expected response, we should define a retry limit\n            logger.write(f'Decision reasoning: {pre_decision_making_reasoning}')\n            # Information summary preparation\n            if use_information_summary and len(memory.get_recent_history(\"decision_making_reasoning\", memory.max_recent_steps)) == memory.max_recent_steps:\n                input = planner.information_summary_.input_map\n                logger.write(f'> Information summary call...')\n                images = memory.get_recent_history('image', config.event_count)"
        },
        {
            "comment": "The code retrieves recent reasoning history and creates an image introduction list with the corresponding reasonings. It then adds this to the input along with previous summarization, task description, and event count. Finally, it calls the information summary function and logs relevant information.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":389-403",
            "content": "                reasonings = memory.get_recent_history('decision_making_reasoning', config.event_count)\n                image_introduction = [{\"path\": images[event_i],\"assistant\": \"\",\"introduction\": 'This is the {} screenshot of recent events. The description of this image: {}'.format(['first','second','third','fourth','fifth'][event_i], reasonings[event_i])} for event_i in range(config.event_count)]\n                input[\"image_introduction\"] = image_introduction\n                input[\"previous_summarization\"] = memory.get_summarization()\n                input[\"task_description\"] = task_description\n                input[\"event_count\"] = str(config.event_count)\n                # >> Calling INFORMATION SUMMARY\n                logger.write(f'>> Calling INFORMATION SUMMARY')\n                data = planner.information_summary(input = input)\n                info_summary = data['res_dict']['info_summary']\n                entities_and_behaviors = data['res_dict']['entities_and_behaviors']\n                logger.write(f'R: Summary: {info_summary}')"
        },
        {
            "comment": "This code is preparing for success detection by creating an input map and adding image introductions to it. It retrieves the most recent images from memory and assigns them to the relevant introductions in the input map. Finally, it updates the \"image_introduction\" field of the input map with the newly created image introductions.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":404-424",
            "content": "                logger.write(f'R: entities_and_behaviors: {entities_and_behaviors}')\n                memory.add_summarization(info_summary)\n            memory.add_recent_history(\"image\", cur_screen_shot_path)\n            # Success detection preparation\n            if use_success_detection:\n                input = planner.success_detection_.input_map\n                image_introduction = [\n                    {\n                        \"introduction\": input[\"image_introduction\"][-2][\"introduction\"],\n                        \"path\": memory.get_recent_history(\"image\", k=2)[0],\n                        \"assistant\": input[\"image_introduction\"][-2][\"assistant\"]\n                    },\n                    {\n                        \"introduction\": input[\"image_introduction\"][-1][\"introduction\"],\n                        \"path\": memory.get_recent_history(\"image\", k=1)[0],\n                        \"assistant\": input[\"image_introduction\"][-1][\"assistant\"]\n                    }\n                ]\n                input[\"image_introduction\"] = image_introduction"
        },
        {
            "comment": "This code is calling the \"success_detection\" function to determine if a task has been completed successfully. It retrieves the previous action and reasoning from memory, then logs the results of the detection. If there's a KeyboardInterrupt, it exits the program.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":426-448",
            "content": "                input[\"task_description\"] = task_description\n                input[\"previous_action\"] = memory.get_recent_history(\"action\", k=1)[-1]\n                input[\"previous_reasoning\"] = memory.get_recent_history(\"decision_making_reasoning\", k=1)[-1]\n                # >> Calling SUCCESS DETECTION\n                logger.write(f'>> Calling SUCCESS DETECTION')\n                data = planner.success_detection(input = input)\n                success = data['res_dict']['success']\n                success_reasoning = data['res_dict']['reasoning']\n                success_criteria = data['res_dict']['criteria']\n                memory.add_recent_history(\"success_detection_reasoning\", success_reasoning)\n                logger.write(f'Success: {success}')\n                logger.write(f'Success criteria: {success_criteria}')\n                logger.write(f'Success reason: {success_reasoning}')\n            gm.store_skills()\n            memory.save()\n        except KeyboardInterrupt:\n            logger.write('KeyboardInterrupt Ctrl+C detected, exiting.')"
        },
        {
            "comment": "The code is setting up the necessary parameters and configurations for a specific software module. It uses an argument parser to handle command-line arguments, sets a fixed seed for reproducibility, and initializes various prompts from different input files. The main functionality is likely related to AI decision making or planning, utilizing different sub-modules with predefined templates for processing inputs.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":449-487",
            "content": "            gm.cleanup_io()\n            videocapture.finish_capture()\n            break\n    gm.cleanup_io()\n    videocapture.finish_capture()\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--providerConfig\",\n        type=str,\n        default=\"./conf/openai_config.json\",\n    )\n    args = parser.parse_args()\n    config.set_fixed_seed()\n    # only change the input for different sub-modules\n    # the tempaltes are now fixed\n    planner_params = {\n        \"__check_list__\": [\n            \"decision_making\",\n            \"gather_information\",\n            \"success_detection\",\n            \"information_summary\",\n            \"gather_text_information\"\n        ],\n        \"prompt_paths\": {\n            \"inputs\": {\n                \"decision_making\": \"./res/prompts/inputs/decision_making.json\",\n                \"gather_information\": \"./res/prompts/inputs/gather_information.json\",\n                \"success_detection\": \"./res/prompts/inputs/success_detection.json\",\n                \"self_reflection\": \"./res/prompts/inputs/self_reflection.json\","
        },
        {
            "comment": "The code defines paths for various JSON and prompt template files used in the application. It also includes a list of skills available in the skill library, and a configuration setting that disables local OCR checks.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":488-505",
            "content": "                \"information_summary\": \"./res/prompts/inputs/information_summary.json\",\n                \"gather_text_information\": \"./res/prompts/inputs/gather_text_information.json\"\n            },\n            \"templates\": {\n                \"decision_making\": \"./res/prompts/templates/decision_making.prompt\",\n                \"gather_information\": \"./res/prompts/templates/gather_information.prompt\",\n                \"success_detection\": \"./res/prompts/templates/success_detection.prompt\",\n                \"self_reflection\": \"./res/prompts/templates/self_reflection.prompt\",\n                \"information_summary\": \"./res/prompts/templates/information_summary.prompt\",\n                \"gather_text_information\": \"./res/prompts/templates/gather_text_information.prompt\"\n            },\n        }\n    }\n    skill_library = ['turn', 'move_forward', 'turn_and_move_forward', 'follow', 'aim', 'shoot', 'shoot_wolves', 'select_weapon', 'select_sidearm', 'fight', 'mount_horse']\n    task_description =  \"\"\n    config.ocr_fully_ban = True # not use local OCR-checks"
        },
        {
            "comment": "This code segment sets OCR (Optical Character Recognition) as disabled, enables skill retrieval, and triggers a pipeline loop with specific parameters: no success detection, self-reflection enabled, and information summary enabled.",
            "location": "\"/media/root/Prima/works/github_code/Cradle/docs/src/prototype_runner.py\":506-509",
            "content": "    config.ocr_enabled = False\n    config.skill_retrieval = True\n    trigger_pipeline_loop(args.providerConfig, planner_params, task_description, skill_library, use_success_detection = False, use_self_reflection = True, use_information_summary = True)"
        }
    ]
}